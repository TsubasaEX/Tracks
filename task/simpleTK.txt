Evelyn
-------------------
v * rebooting manual/scripts
v * tags for each components
v * AlertManager - up/volume/ -> group
v * add installFromRepo.sh
v * Monitoring Manual - alertManager (silence/ingress)
v * values.yaml - repository/tags
v * rmm-event-datasource
v * password
v * binary exec for shutdown/reboot
v * docker-registry - > minio(bucket: images) -> modify minio storage
v * docker-registry - > secret
v * ceph-exporter
v * ceph-dashboard
v * rolling update
v * maunaul - ceph-dashboard
v * podPriority
- * restartPolicy
v * kubeapps - > minio/disable public repos
v * kubeapps - > if deployment fails? Not Ready
* kubeapps 
	- cronJob
	- README
	- Manual(admin/user)
	- RBAC
* kubeapps - > useful charts?
* kubeapps - > service catalog
* add chart icon (catalog)
v * add chart icon (applications)
v * Admin/User Manual - > App Management (Permissiom/Deploy/Delete/Upgrade/Add Repo/Refresh)
*1.如何新增plugin (root only)
	- online
	- offline
* EMQX
	- stress test
	- 2T (400/75/1)
		QoS 0- lost data? 
	- 2T (600/75/1)
		QoS 0- lost data? 
	- 2T (800/75/1)
		QoS 0- lost data? 
	x - 10T (400/75/1)
* event-exporter
* helm-exporter
* mongo capped
	- 200MB (2T)
* loki-stack - > retention period (10T)
* loki-stack - > add labels for all apps


^^^^^
x* WeaveScope -> root only
	- basic-auth
	- scope-http-statistics
	- scope-volume-count
	- scope-iowait
	- snapshot
* efk-stack -> 1Node? 2Nodes? 
* LDAP
	- Dex (app+k8s)
	- Cluster auth with GitHub, Dex and Gangway
	- Guard (webhook - app)
	- kuberos
	- Keycloak
* Vault
	- Encryption as a service 
	    - prior to 1.13 (--experimental-encryption-provider-config)
		- KMS (Transit Secrets Engine)-> 1Node? 2Nodes? on k8s? outside?
	- Secrets Management
		- Accessing and Storing Secrets
			- Static Secrets (KV Secrets Engine)
			- service accounts
			- Dynamic Secrets? (Secrets Engines)
	- Encryption as a Service
		- Transit secret engine
* Admission Controller
	- Pod Security Policy to system:authenticated
		- privileged: false
		- hostNetwork: false
		- allowPrivilegeEscalation: false
		- defaultAllowPrivilegeEscalation: false
		- hostPID: false
		- hostIPC: false
		- hostNetwork: false
		-  runAsUser:
			rule: RunAsAny
		fsGroup:
			rule: RunAsAny
		seLinux:
			rule: RunAsAny
		supplementalGroups:
			rule: RunAsAny
		- volumes: '*'
	- OPA/OPA Gatekeeper
		- Allowed hostname *.example.com (ingress whitelist)
		- Fobidden conflict hostname
		- PV Size Limitation
		- Allowed Private Docker Repository
		- Report Audit Results
* Network Policy by each pods
	- apt-get install dnsutils (install nslookup)
	- nslookup <service>.<namespace>
	- CIDR in calico pod(Allow Service Range)
	- ingress  (forbid outbound traffic coming into)
	- egress (forbid outbound traffic coming out)
* Mongo DB rotation
* Searchlight
* Mongo Sharding w/ replica -> 3 Nodes
* Rancher -> replace kube-dashboard
* minio-exporter
* minio-dashboard
* Monitoring Manual - ceph/mino/loki
* Monitoring Manual - WeaveScope
* k3s


Terry
-------------------
v * add rbac - one/two
v * add rebooting scripts
v * reorg - install scripts
v * kubernetes dashboard - problem login with viewer/ -> --authentication-mode=basic
v * *(dashboard?) config or config-root — /root/config from /root/.kube — /// sudo chattr -V +ia config

v * add /etc/dss/charts
v * uploading charts - /etc/dss/charts/pushCharts.sh - chartmuseum

v * 10T Deployment - Mongo(1.5 hr)

v * add /etc/dss/images
v * uploading images - /etc/dss/images/pushImages.sh - docker-registry

v * pushImage.sh
v * add DNS namespace in /etc/resolv.conf

v * PodPriority 
    - Guaranteed Scheduling For Critical Add-On Pods
	- https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/
	- https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
	- Nginx/CoreDNS	
	- Docker Registry/Chartmuseum

v * recover docker-registry/images
v * HDD/SSD systeminfo
v * ceph account (default)
* recovery documents - admin
	- docker-registry
	- chartmuseum
* remove sudo from ubuntu
* replace README
* Default Storage Class
* Encrypting Secret Data at Rest
	- Kubernetes version 1.13.0 or later is required
	--experimental-encryption-provider-config (prior to 1.13)
* kube-router
* ceph balancer		
* helmfile/helmsman
* base image - ubuntu
	
? * dynamic scaling script - PV/helm upgrade

* 10T*2 Deployment - Mongo (require time?)

* scaling scripts - helm upgade
* scaling manual
* CI/CD

v * update ADV_Param
* technical manual - architecture (ip)
^^^^^
* helm backup - /etc/backups
* IP Change
* HA Testing

Alex
-------------------


Newbie
-------------------
* Rook
	- Ceph
	- NFS
* restic
* velero
* volume snaptshot
* NAS Support

Me
-------------------
v * chartmuseum
v * Cluster Management Manual - kubernets dashboard
v * Redmine
* eflow
v * update strategy: Rolling Update
	- maxUnavailable: 0
	- http://rahmonov.me/posts/zero-downtime-deployment-with-kubernetes/
v * PodPriority
v * restartPolicy
v * README
x * TPM
* volume snaptshot
	- Alpha - k8s v1.12
* kubeconfig
	- admin.conf (if no config?)
	- new user?


HA Issues
-------------------
1. No nodeSelector
2. Adopt packages of single instance
3. Ceph Object Replicas (3) - > All PV HA(Mongo/Postgres/RabbitMQ/Prometheus) -> Still single instances
4. memory specs?
5. App HA or Ceph Replicas?
6. Would all replicas deleted by helm delete?
7. Would mongo still keep data if one osd down when replica 3?
8. CRUSH Rules - > 3 replicas on 3 hosts
9. ceph osd pool set data min_size 3 / ceph osd pool set data size 3 
10. Loadbalancer (Keepalived ) for 3 master 


Kubernetes Test
-------------------
1. Reliability/Stability
	- Stress 
		- *ClusterLoader
			- https://github.com/kubernetes/perf-tests/tree/master/clusterloader2
	- Chaos
		- Kube Monkey
			- https://github.com/asobti/kube-monkey
			- https://www.gremlin.com/chaos-monkey/chaos-monkey-alternatives/kubernetes/
		- *Chaoskube
			- https://kubedex.com/resource/chaoskube/
			- https://hub.kubeapps.com/charts/stable/chaoskube
		- Chaos Toolkit
			- https://github.com/chaostoolkit/chaostoolkit-kubernetes
		- *powerfulseal
			- https://github.com/bloomberg/powerfulseal
		
2.load-test, benchmark, framework or other tools
	- *test-infra
		- https://github.com/kubernetes/test-infra
	- perf-tests
		- https://github.com/kubernetes/perf-tests
	- Sonobuoy
		- https://github.com/heptio/sonobuoy
		
3. synthetic testing		
	- *KuberHealthy	
		- Easy synthetic testing for Kubernetes clusters. Supplements other solutions like Prometheus nicely.
		- https://github.com/Comcast/kuberhealthy		
		
4. e2e test
	- https://github.com/kubernetes/kubernetes/tree/master/test/e2e

5. Validate a kops cluster.
	- https://github.com/kubernetes/kops/blob/master/docs/cli/kops_validate.md
	


